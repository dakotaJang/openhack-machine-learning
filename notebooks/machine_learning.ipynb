{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.image\n",
    "import keras_preprocessing.image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessImages():\n",
    "    for root, dirs, files in os.walk(\"/jupyter/Classification/assets/gear_images\"):\n",
    "        if(len(dirs)==0):\n",
    "            p_dir = os.path.join(root).replace('gear_images','p_gear_images')\n",
    "            if not os.path.exists(p_dir):\n",
    "                os.makedirs(p_dir)\n",
    "            print('walking ' + os.path.basename(root))\n",
    "            for name in files:\n",
    "                image = keras_preprocessing.image.load_img(os.path.join(root, name), target_size=(128,128))\n",
    "                image.save(os.path.join(p_dir, name))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking boots\n",
      "walking hardshell_jackets\n",
      "walking axes\n",
      "walking carabiners\n",
      "walking helmets\n",
      "walking harnesses\n",
      "walking rope\n",
      "walking crampons\n",
      "walking gloves\n",
      "walking tents\n",
      "walking insulated_jackets\n",
      "walking pulleys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessImages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImages():\n",
    "    images = []\n",
    "    labels = []\n",
    "    for root, dirs, files in os.walk(\"/jupyter/Classification/assets/p_gear_images\"):\n",
    "        if(len(dirs)==0):\n",
    "            print('walking ' + os.path.basename(root))\n",
    "            for name in files:\n",
    "                labels.append(os.path.basename(root))\n",
    "                images.append(matplotlib.image.imread(os.path.join(root, name)))\n",
    "    print('loaded all images')\n",
    "    x = np.array(images)\n",
    "    y = np.array(labels)\n",
    "    dictionary = list(set(labels))\n",
    "    dictionary.sort()\n",
    "    for label in dictionary:\n",
    "        y[y==label]=dictionary.index(label)\n",
    "    return x, y, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking boots\n",
      "walking hardshell_jackets\n",
      "walking axes\n",
      "walking carabiners\n",
      "walking helmets\n",
      "walking harnesses\n",
      "walking rope\n",
      "walking crampons\n",
      "walking gloves\n",
      "walking tents\n",
      "walking insulated_jackets\n",
      "walking pulleys\n",
      "loaded all images\n",
      "['axes', 'boots', 'carabiners', 'crampons', 'gloves', 'hardshell_jackets', 'harnesses', 'helmets', 'insulated_jackets', 'pulleys', 'rope', 'tents']\n"
     ]
    }
   ],
   "source": [
    "# loade images and labels\n",
    "x,y,dictionary = loadImages()\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (1697, 128, 128, 3)\n",
      "1697 train samples\n",
      "425 test samples\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "num_classes = len(dictionary)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(128, 128, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1697 samples, validate on 425 samples\n",
      "Epoch 1/10\n",
      "1697/1697 [==============================] - 66s 39ms/step - loss: 1.8293 - acc: 0.4543 - val_loss: 0.9070 - val_acc: 0.7506\n",
      "Epoch 2/10\n",
      "1697/1697 [==============================] - 65s 39ms/step - loss: 0.7893 - acc: 0.7413 - val_loss: 0.6215 - val_acc: 0.8024\n",
      "Epoch 3/10\n",
      "1697/1697 [==============================] - 66s 39ms/step - loss: 0.4994 - acc: 0.8427 - val_loss: 0.4189 - val_acc: 0.8776\n",
      "Epoch 4/10\n",
      "1697/1697 [==============================] - 65s 39ms/step - loss: 0.3755 - acc: 0.8886 - val_loss: 0.3656 - val_acc: 0.8988\n",
      "Epoch 5/10\n",
      "1697/1697 [==============================] - 65s 38ms/step - loss: 0.3032 - acc: 0.9110 - val_loss: 0.3449 - val_acc: 0.8965\n",
      "Epoch 6/10\n",
      "1697/1697 [==============================] - 66s 39ms/step - loss: 0.2031 - acc: 0.9393 - val_loss: 0.3263 - val_acc: 0.9059\n",
      "Epoch 7/10\n",
      "1697/1697 [==============================] - 68s 40ms/step - loss: 0.1605 - acc: 0.9511 - val_loss: 0.4579 - val_acc: 0.8659\n",
      "Epoch 8/10\n",
      "1697/1697 [==============================] - 75s 44ms/step - loss: 0.2677 - acc: 0.9187 - val_loss: 0.3515 - val_acc: 0.9153\n",
      "Epoch 9/10\n",
      "1697/1697 [==============================] - 86s 51ms/step - loss: 0.1297 - acc: 0.9588 - val_loss: 0.3729 - val_acc: 0.9012\n",
      "Epoch 10/10\n",
      "1697/1697 [==============================] - 93s 55ms/step - loss: 0.1244 - acc: 0.9629 - val_loss: 0.3320 - val_acc: 0.9224\n",
      "Test loss: 0.3320194466675029\n",
      "Test accuracy: 0.922352941456963\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('/jupyter/Classification/cnn_model_gears.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helmets\n"
     ]
    }
   ],
   "source": [
    "# model = load_model('/jupyter/Classification/cnn_model_gears.h5')\n",
    "# https://shop.epictv.com/sites/default/files/ae42ad29e70ba8ce6b67d3bdb6ab5c6e.jpeg\n",
    "image = keras_preprocessing.image.load_img('/jupyter/Classification/assets/untrained_image.jpeg', target_size=(128,128))\n",
    "dictionary = ['axes', 'boots', 'carabiners', 'crampons', 'gloves', 'hardshell_jackets', 'harnesses', 'helmets', 'insulated_jackets', 'pulleys', 'rope', 'tents']\n",
    "print(dictionary[model.predict(np.array([np.array(image)])).argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
